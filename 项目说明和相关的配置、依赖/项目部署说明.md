### 设置hive
##### 1.&nbsp;&nbsp; 在每个用到sparkSQL的项目中的resources下的hive-site.xml中设置远程hive metastroe节点的地址
##### 2.&nbsp;&nbsp; 默认是在本地运行spark，为了能用IDE，调试方便，也可以自己打成jar在linux节点上用命令运行
### 设置flume
##### 1.&nbsp;&nbsp;在flume的类路径下将hbase的hbase-site.xml复制进去
##### 2.&nbsp;&nbsp;3个agent的sink是hbaseSink，所以要在flume的类路径下也就是conf下加入hbase的hbase-site.xml
##### 3.&nbsp;&nbsp;4个flume的配文分别代表3个agent进程，在node6中并行启动。一个collector进程，在node7
##### 4.&nbsp;&nbsp;custom_flume_hbase_Serializer.zip 是Collector的自定义序列化器的源码包，解压后可以直接导入为idea的maven项目，使用时需要打成jar，注意打jar时相关的jar的scope全都要设置为provided，因为远程的flume已经关联了hbase的jar，不需要再打到jar中，如果打进去会报错
